[core]
# The folder where your DAGs are stored
dags_folder = /opt/airflow/dags

# The base log folder
base_log_folder = /opt/airflow/logs

# Airflow executor - KubernetesExecutor for running tasks in separate pods
executor = KubernetesExecutor

# SQL Alchemy connection string for metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load example DAGs
load_examples = False

# Default timezone
default_timezone = utc

[webserver]
# The base url of your website
base_url = http://localhost:8080

# Default UI timezone
default_ui_timezone = UTC

# The port on which to run the web server
web_server_port = 8080

# Number of workers to run the Gunicorn web server
workers = 2

# Secret key for Flask
secret_key = changeme_to_a_secure_random_key

[scheduler]
# Number of seconds after which a DAG run execution is considered failed
dagrun_timeout = 600

# How often (in seconds) to scan the DAGs directory for new files
dag_dir_list_interval = 300

[kubernetes]
# The namespace where Airflow runs
namespace = airflow

# The Docker image to use for tasks
worker_container_repository = your-registry/airflow
worker_container_tag = 3.1.0-py3.12

# Resources for worker pods
worker_pods_creation_batch_size = 5

# Delete worker pods on success/failure
delete_worker_pods = True
delete_worker_pods_on_failure = False

# Path to Kubernetes config file
kube_config_file = 

# Worker pod labels
worker_pods_labels = {"app": "airflow-worker", "version": "3.1.0"}

[logging]
# Logging level
logging_level = INFO

# Log format
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s

# Colored logs in console
colored_console_log = True

[api]
# How to authenticate users of the API
auth_backends = airflow.api.auth.backend.basic_auth

[operators]
# Default owner assigned to new operators
default_owner = airflow

[email]
# Email backend to use
email_backend = airflow.utils.email.send_email_smtp

[smtp]
# If you want airflow to send emails on failures, configure SMTP settings
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 25
smtp_mail_from = airflow@example.com
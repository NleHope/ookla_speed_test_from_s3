apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: ookla-transform-job
  namespace: default

spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: spark-s3:latest
  imagePullPolicy: IfNotPresent
  build: 
    context: ../..
    dockerfile: Dockerfile.spark
  mainApplicationFile: local:///opt/spark/app/cleaning.py
  arguments:
    - "s3a://bronze/pump/2019-q1/"
    - "s3a://silver/pump/2019-q1/"
  
  sparkVersion: "3.5.1"

  sparkConf:
    # Executor configuration
    spark.driver.cores: "1"
    spark.driver.memory: "2g"

    spark.sql.shuffle.partitions: "8"

    # MinIO S3 endpoint configuration
    spark.hadoop.fs.s3a.endpoint: "http://host.minikube.internal:9000"
    spark.hadoop.fs.s3a.access.key: "minio_access_key"
    spark.hadoop.fs.s3a.secret.key: "minio_secret_key"
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    spark.hadoop.fs.s3a.connection.ssl.enabled: "false"
    
    # S3A optimizations for transformation workload
    spark.hadoop.fs.s3a.multipart.size: "104857600"  # 100MB
    spark.hadoop.fs.s3a.fast.upload: "true"
    spark.hadoop.fs.s3a.fast.upload.buffer: "disk"
    spark.hadoop.fs.s3a.connection.maximum: "20"
    spark.hadoop.fs.s3a.threads.max: "10"
    spark.hadoop.fs.s3a.attempts.maximum: "5"
    
    # Adaptive query execution for better performance
    spark.sql.adaptive.enabled: "true"
    spark.sql.adaptive.coalescePartitions.enabled: "true"
    spark.sql.adaptive.skewJoin.enabled: "true"
    
    # Memory and shuffle optimization
    spark.memory.fraction: "0.6"
    spark.memory.storageFraction: "0.3"

    
    # Python environment
    spark.pyspark.python: "/usr/bin/python3"
    spark.pyspark.driver.python: "/usr/bin/python3"
    
    # Kubernetes volumes for local scratch space
    spark.kubernetes.driver.volumes.emptyDir.spark-local-dir-1.mount.path: "/tmp/spark-local"
    spark.kubernetes.driver.volumes.emptyDir.spark-local-dir-1.sizeLimit: "10Gi"
    spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-1.mount.path: "/tmp/spark-local"
    spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-1.sizeLimit: "10Gi"

  hadoopConf:
    fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    fs.s3a.endpoint: "http://host.minikube.internal:9000"
    fs.s3a.access.key: "minio_access_key"
    fs.s3a.secret.key: "minio_secret_key"
    fs.s3a.path.style.access: "true"
    fs.s3a.connection.ssl.enabled: "false"
  
  deps:
    jars:
      - local:///opt/spark/jars/hadoop-aws-3.3.4.jar
      - local:///opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar

  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "2g"
    labels:
      version: 3.5.1
      app: ookla-transform
      layer: silver
    serviceAccount: spark
    env:
      - name: MINIO_ENDPOINT
        value: "http://host.minikube.internal:9000"
      - name: MINIO_ACCESS_KEY
        value: "minio_access_key"
      - name: MINIO_SECRET_KEY
        value: "minio_secret_key"
      - name: PYSPARK_PYTHON
        value: "/usr/bin/python3"
      - name: PYTHONPATH
        value: "/opt/spark/app"

  executor:
    cores: 1
    coreLimit: "1100m"
    instances: 2
    memory: "2g"
    labels:
      version: 3.5.1
      app: ookla-transform
      layer: silver
    serviceAccount: spark
    env:
      - name: MINIO_ENDPOINT
        value: "http://host.minikube.internal:9000"
      - name: MINIO_ACCESS_KEY
        value: "minio_access_key"
      - name: MINIO_SECRET_KEY
        value: "minio_secret_key"
      - name: PYSPARK_PYTHON
        value: "/usr/bin/python3"
      - name: PYTHONPATH
        value: "/opt/spark/app"

  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 5
    onSubmissionFailureRetryInterval: 20
  
  # monitoring:
  #   exposeDriverMetrics: true
  #   exposeExecutorMetrics: true
  #   prometheus:
  #     jmxExporterJar: "/prometheus/jmx_prometheus_javaagent-0.17.0.jar"
  #     port: 8090
